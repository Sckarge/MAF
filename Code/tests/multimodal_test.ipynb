{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from typing import Optional\n",
        "import math"
      ],
      "metadata": {
        "id": "OMRs2926WZ-F"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.models.mbart.modeling_mbart import MBartPreTrainedModel,MBartDecoder,MBartLearnedPositionalEmbedding,MBartEncoderLayer,shift_tokens_right\n",
        "from transformers.models.mbart.configuration_mbart import MBartConfig\n",
        "#Missing _expand_mask\n",
        "\n",
        "from transformers import MBart50TokenizerFast\n",
        "\n",
        "from transformers.modeling_outputs import (\n",
        "    BaseModelOutput,\n",
        "    Seq2SeqLMOutput,\n",
        "    Seq2SeqModelOutput\n",
        ")\n",
        "\n",
        "from torch.nn import TransformerEncoder\n"
      ],
      "metadata": {
        "id": "3xhkaN96bj2N"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultimodalMBartEncoder(MBartPreTrainedModel):\n",
        "\n",
        "  def __init__(self,config:MBartConfig,embed_tokens: Optional[nn.Embedding] = None):\n",
        "\n",
        "    self.dropout = config.dropout\n",
        "    self.layerdrop = config.encoder_layerdrop\n",
        "    embed_dim = config.d_model\n",
        "    self.padding_idx = config.pad_token_id\n",
        "    self.max_source_positions = config.max_position_embeddings\n",
        "    self.embed_scale = math.sqrt(embed_dim) if config.scale_embedding else 1.0\n",
        "\n",
        "    if embed_tokens is not None:\n",
        "      self.embed_tokens = embed_tokens\n",
        "    else:\n",
        "      self.embed_tokens = nn.Embedding(config.vocab_size, embed_dim, self.padding_idx)\n",
        "\n",
        "    print()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1Jv_xkCrhp15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwQ9ZBP5WElu"
      },
      "outputs": [],
      "source": [
        "class MultimodalMBartModel(MBartPreTrainedModel):\n",
        "\n",
        "  def __init__(self,config:MBartConfig):\n",
        "\n",
        "    super().__init__(config)\n",
        "\n",
        "    padding_idx, vocab_size = config.pad_token_id, config.vocab_size\n",
        "    self.shared = nn.Embedding(vocab_size,config.d_model,padding_idx)\n",
        "\n",
        "    self.encoder = MultimodalMBartEncoder(config,self.shared)\n",
        "    self.decoder = MBartDecoder(config,self.shared)\n",
        "\n",
        "    self.init_weights()\n",
        "\n"
      ]
    }
  ]
}