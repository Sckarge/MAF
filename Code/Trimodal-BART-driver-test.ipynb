{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Obtaining dependency information for torch from https://files.pythonhosted.org/packages/2c/df/5810707da6f2fd4be57f0cc417987c0fa16a2eecf0b1b71f82ea555dc619/torch-2.2.1-cp311-cp311-manylinux1_x86_64.whl.metadata\n",
      "  Using cached torch-2.2.1-cp311-cp311-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: filelock in /home/sckarge/anaconda3/lib/python3.11/site-packages (from torch) (3.9.0)\n",
      "Collecting typing-extensions>=4.8.0 (from torch)\n",
      "  Obtaining dependency information for typing-extensions>=4.8.0 from https://files.pythonhosted.org/packages/f9/de/dc04a3ea60b22624b51c703a84bbe0184abcd1d0b9bc8074b5d6b7ab90bb/typing_extensions-4.10.0-py3-none-any.whl.metadata\n",
      "  Using cached typing_extensions-4.10.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: sympy in /home/sckarge/anaconda3/lib/python3.11/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in /home/sckarge/anaconda3/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/sckarge/anaconda3/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /home/sckarge/anaconda3/lib/python3.11/site-packages (from torch) (2023.4.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
      "  Obtaining dependency information for nvidia-cuda-nvrtc-cu12==12.1.105 from https://files.pythonhosted.org/packages/b6/9f/c64c03f49d6fbc56196664d05dba14e3a561038a81a638eeb47f4d4cfd48/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
      "  Obtaining dependency information for nvidia-cuda-runtime-cu12==12.1.105 from https://files.pythonhosted.org/packages/eb/d5/c68b1d2cdfcc59e72e8a5949a37ddb22ae6cade80cd4a57a84d4c8b55472/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
      "  Obtaining dependency information for nvidia-cuda-cupti-cu12==12.1.105 from https://files.pythonhosted.org/packages/7e/00/6b218edd739ecfc60524e585ba8e6b00554dd908de2c9c66c1af3e44e18d/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
      "  Obtaining dependency information for nvidia-cudnn-cu12==8.9.2.26 from https://files.pythonhosted.org/packages/ff/74/a2e2be7fb83aaedec84f391f082cf765dfb635e7caa9b49065f73e4835d8/nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata\n",
      "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
      "  Obtaining dependency information for nvidia-cublas-cu12==12.1.3.1 from https://files.pythonhosted.org/packages/37/6d/121efd7382d5b0284239f4ab1fc1590d86d34ed4a4a2fdb13b30ca8e5740/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata\n",
      "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
      "  Obtaining dependency information for nvidia-cufft-cu12==11.0.2.54 from https://files.pythonhosted.org/packages/86/94/eb540db023ce1d162e7bea9f8f5aa781d57c65aed513c33ee9a5123ead4d/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata\n",
      "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
      "  Obtaining dependency information for nvidia-curand-cu12==10.3.2.106 from https://files.pythonhosted.org/packages/44/31/4890b1c9abc496303412947fc7dcea3d14861720642b49e8ceed89636705/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata\n",
      "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
      "  Obtaining dependency information for nvidia-cusolver-cu12==11.4.5.107 from https://files.pythonhosted.org/packages/bc/1d/8de1e5c67099015c834315e333911273a8c6aaba78923dd1d1e25fc5f217/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata\n",
      "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
      "  Obtaining dependency information for nvidia-cusparse-cu12==12.1.0.106 from https://files.pythonhosted.org/packages/65/5b/cfaeebf25cd9fdec14338ccb16f6b2c4c7fa9163aefcf057d86b9cc248bb/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata\n",
      "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.19.3 (from torch)\n",
      "  Obtaining dependency information for nvidia-nccl-cu12==2.19.3 from https://files.pythonhosted.org/packages/38/00/d0d4e48aef772ad5aebcf70b73028f88db6e5640b36c38e90445b7a57c45/nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata\n",
      "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
      "  Obtaining dependency information for nvidia-nvtx-cu12==12.1.105 from https://files.pythonhosted.org/packages/da/d3/8057f0587683ed2fcd4dbfbdfdfa807b9160b809976099d36b8f60d08f03/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata\n",
      "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==2.2.0 (from torch)\n",
      "  Obtaining dependency information for triton==2.2.0 from https://files.pythonhosted.org/packages/bd/ac/3974caaa459bf2c3a244a84be8d17561f631f7d42af370fc311defeca2fb/triton-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached triton-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
      "  Obtaining dependency information for nvidia-nvjitlink-cu12 from https://files.pythonhosted.org/packages/1e/07/bf730d44c2fe1b676ad9cc2be5f5f861eb5d153fb6951987a2d6a96379a9/nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl.metadata\n",
      "  Using cached nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/sckarge/anaconda3/lib/python3.11/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/sckarge/anaconda3/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Downloading torch-2.2.1-cp311-cp311-manylinux1_x86_64.whl (755.6 MB)\n",
      "\u001b[2K   \u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/755.6 MB\u001b[0m \u001b[31m729.9 kB/s\u001b[0m eta \u001b[36m0:17:12\u001b[0m^C\n",
      "\u001b[2K   \u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/755.6 MB\u001b[0m \u001b[31m729.4 kB/s\u001b[0m eta \u001b[36m0:17:13\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install torch\n",
    "# %pip install pandas\n",
    "# %pip install nltk\n",
    "# %pip install rouge-score==0.0.4\n",
    "# %pip uninstall -y transformers\n",
    "# %pip install transformers\n",
    "# %pip freeze\n",
    "# %pip install --upgrade pip setuptools\n",
    "# %pip install --no-cache-dir transformers==4.10.0\n",
    "# %pip install --index-url=https://pypi.python.org/simple/transformers==4.10.0\n",
    "# %pip install git+https://github.com/huggingface/transformers\n",
    "# %pip install tokenizers --no-index --find-links https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import warnings\n",
    "import logging\n",
    "import gc\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "from typing import Optional\n",
    "from datetime import datetime\n",
    "\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from rouge_score.rouge_scorer import RougeScorer\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_COLUMN = 'code_mixed_explanation'\n",
    "TEXT_INPUT_PATH = '../Data/Text/'\n",
    "ACOUSTIC_INPUT_PATH = '../Data/Audio/'\n",
    "VISUAL_INPUT_PATH = '../Data/Video/'\n",
    "\n",
    "MODEL_OUTPUT_DIR = '../models/MAF-TAV-BART/'\n",
    "RESULT_OUTPUT_DIR = '../results/MAF-TAV-BART/'\n",
    "\n",
    "LOWERCASE_UTTERANCES = False\n",
    "UNFOLDED_DIALOGUE = True\n",
    "\n",
    "if UNFOLDED_DIALOGUE:\n",
    "    SOURCE_COLUMN = 'dialogue'\n",
    "else:\n",
    "    SOURCE_COLUMN_1 = 'target'\n",
    "    SOURCE_COLUMN_2 = 'context'\n",
    "    \n",
    "\n",
    "\n",
    "SOURCE_MAX_LEN = 480\n",
    "TARGET_MAX_LEN = 50\n",
    "MAX_UTTERANCES = 25\n",
    "\n",
    "ACOUSTIC_DIM = 154\n",
    "ACOUSTIC_MAX_LEN = 600\n",
    "\n",
    "VISUAL_DIM = 2048\n",
    "VISUAL_MAX_LEN = 96 \n",
    "\n",
    "BATCH_SIZE = 4\n",
    "MAX_EPOCHS = 60\n",
    "\n",
    "BASE_LEARNING_RATE = 5e-6\n",
    "NEW_LEARNING_RATE = 5e-5\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "NUM_BEAMS = 5\n",
    "EARLY_STOPPING = True\n",
    "NO_REPEAT_NGRAM_SIZE = 3\n",
    "\n",
    "EARLY_STOPPING_THRESHOLD = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(seed: int):\n",
    "    \"\"\"\n",
    "    Helper function to seed experiment for reproducibility.\n",
    "    If -1 is provided as seed, experiment uses random seed from 0~9999\n",
    "    Args:\n",
    "        seed (int): integer to be used as seed, use -1 to randomly seed experiment\n",
    "    \"\"\"\n",
    "    print(\"Seed: {}\".format(seed))\n",
    "\n",
    "\n",
    "    # These lines below ensure determinism by turning off some optimizations\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.enabled = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    if seed == -1:\n",
    "        seed = random.randint(0, 9999)\n",
    "\n",
    "    # We set all seeds in libraries to our seed\n",
    "    \n",
    "    random.seed(seed) #python built-in\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed) #seed for hash based ops in python\n",
    "    np.random.seed(seed) #numpy random gen\n",
    "    torch.manual_seed(seed) # torch CPU random generator\n",
    "    torch.cuda.manual_seed(seed) # current gpu rand gen\n",
    "    torch.cuda.manual_seed_all(seed) # all gpus rand gen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "    print(\"Using GPU\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 42\n"
     ]
    }
   ],
   "source": [
    "set_random_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "\n",
    "from typing import Any, Callable, Dict, Iterable, List, Optional, Tuple, Union\n",
    "\n",
    "from transformers.modeling_utils import PreTrainedModel, unwrap_model\n",
    "\n",
    "from transformers import (\n",
    "    BartTokenizerFast,\n",
    "    AdamW\n",
    ")\n",
    "\n",
    "from transformers.models.bart.configuration_bart import BartConfig\n",
    "\n",
    "# from transformers.models.bart.modeling_bart import (\n",
    "#     BartPretrainedModel,\n",
    "#     BartDecoder,\n",
    "#     BartLearnedPositionalEmbedding,\n",
    "#     BartEncoderLayer,\n",
    "#     shift_tokens_right,\n",
    "#     _make_causal_mask,\n",
    "#     _expand_mask \n",
    "# )\n",
    "\n",
    "\n",
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutput,\n",
    "    Seq2SeqLMOutput,\n",
    "    Seq2SeqModelOutput\n",
    ")\n",
    "\n",
    "\n",
    "# from transformer_encoder import TransformerEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_kwargs = {\n",
    "        'num_beams': NUM_BEAMS,\n",
    "        'max_length': TARGET_MAX_LEN,\n",
    "        'early_stopping': EARLY_STOPPING,\n",
    "        'no_repeat_ngram_size': NO_REPEAT_NGRAM_SIZE\n",
    "    }\n",
    "    \n",
    "    train(model=MODEL,\n",
    "          tokenizer=TOKENIZER,\n",
    "          train_data_loader=train_dataset,\n",
    "          val_data_loader=val_dataset,\n",
    "          test_data_loader=test_dataset,\n",
    "          base_learning_rate=BASE_LEARNING_RATE,\n",
    "          new_learning_rate=NEW_LEARNING_RATE,\n",
    "          weight_decay=WEIGHT_DECAY,\n",
    "          **gen_kwargs)\n",
    "    \n",
    "    print(\"Model Trained!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
